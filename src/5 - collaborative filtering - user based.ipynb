{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db7a2b2-5dbf-40f6-96b4-fb135e7c0bbb",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1 id='content-based-filtering' style='color:#7159c1; font-size:350%'>Collaborative Filtering</h1>\n",
    "    <i style='font-size:125%'>Recommendations of Items from Similar Users</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- 🧑‍🤝‍🧑 Content-Based Filtering Problems\n",
    "- 🧑‍🤝‍🧑 Collaborative Filtering\n",
    "- 🧑‍🤝‍🧑 User-Based Approach\n",
    "- 🧑‍🤝‍🧑 K-Nearest Neighbors\n",
    "- 🧑‍🤝‍🧑 K-Nearest Neighbors Basic VS K-Nearest Neighbors With Means VS K-Nearest Neighbors With Z-Score\n",
    "- 🧑‍🤝‍🧑 Grid Search CV VS Randomized Search CV\n",
    "- 🧑‍🤝‍🧑 Hands-on\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f08f2-8ac2-42d6-bcd5-bfd329df2c97",
   "metadata": {},
   "source": [
    "<h1 id='0-content-based-filtering-problems' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>🧑‍🤝‍🧑 | Content-Based Filtering Problems</h1>\n",
    "\n",
    "In the previous two notebooks, we dived into Content-Based Filtering with Plot Description and Metadatas approach and got better recommendations results!!\n",
    "\n",
    "Nonetheless, you may be wondering: *\"Okay, where is the catch? Is this method really perfect? Are there any problems with it?\"*. And yes, even though giving better results, there are some cons on Content-Based Filtering.\n",
    "\n",
    "The first problem is that the recommendations are based on similiar items regardless the user tastes. Picture this, if user A and user B are into Mob Psycho 100, they both will receive the same similar animes recommendations, regardless their animes tastes and, consequently, a Recommendation Bubble is created.\n",
    "\n",
    "Besides, people tastes change over the time, so, even though user A is into shounen animes like Mob Psycho 100 today, in a few weeks this very user can be into slice-of-life animes and, since the given recommendations will be using Mob Psycho 100 as a parameter, the user will not receive any slice-of-life animes recommendations, leading to the user search for another platform to watch what he is looking for.\n",
    "\n",
    "Thus, in order to minimize these problems, a new recommendation method has been made up: `Collaborative Filtering`!! Let's find out what it is and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13085e0e-900f-4ed6-a50c-fdc45b33d8d1",
   "metadata": {},
   "source": [
    "<h1 id='1-collaborative-filtering' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>🧑‍🤝‍🧑 | Collaborative Filtering</h1>\n",
    "\n",
    "`Collaborative Filtering` reccomends animes that similar users liked, being able to get closer to the real users tastes. If you use Netflix, you probably already stumbled upon to some series marked as `For you`. If that's so, congrats, that is a real-world Collaborative Filtering Recommendation!! To make things even clearer, assume that two similar users, user A and user B, like Demon Slayer, and user B is also into Grand Blue, so the platform will recommend Grand Blue to user A.\n",
    "\n",
    "Besides, this Filtering has two modes: 1) `User-Based`, where the user receives recommendations from items that similar users liked; and 2) `Item-Based`, where the user receives recommendations from items that similar users liked and the current user may well rate the recommended item. Both of them follow the `Memory/Neighborhood Logic`.\n",
    "\n",
    "\n",
    "About the advantages:\n",
    "\n",
    "> **Better Recommendations** - `since it recommends animes that similar users liked, this system method tends to get closer to the user tastes when compared to Content-Based and Demographic Filtering`;\n",
    "\n",
    "> **Community Tastes** - `it recommends similar items that similar users liked`;\n",
    "\n",
    "> **Personalized Recommendations** - `even though two users are searching for recommendations using the same anime, such as Mob Psycho 100, both of them will receive different recommendations due to their tastes`;\n",
    "\n",
    "> **Low Bubble of Recommendations** - `consequently, the probability of a Bubble of Recommendations be created is low and, even if one is created, it will be small`.\n",
    "\n",
    "<br />\n",
    "\n",
    "Disadvantages-wise:\n",
    "\n",
    "> **More Data Required** - `ir order to get closer to users tastes, in addition to having animes data, it is needed to have users data, such as their ratings on previously watched animes`;\n",
    "\n",
    "> **Few Data Available for Items** - `new animes will have few ratings from users, leading to outliers`;\n",
    "\n",
    "> **Bubble of Recommendations** - `even though the probability of a small Bubble of Recommendations be created is low, there is yet the risk of it be happening`;\n",
    "\n",
    "> **Outliers** - `it is needed to add a cut-off of users ratings and mean rating score by user in order to avoid outliers in the recommendations. For instance, consider that user A rated 100 animes with 1 score and the very user mean score of all rated animes is 1.5, it means that the user bad rated all animes he watched and, consequently, may be up no good in the platform giving outliers to the ratings`;\n",
    "\n",
    "> **More Computational Cost and Power** - `Collaborative-Filtering Algorithms are more complex and sofisticated to the previously ones, then, more computational cost and power is needed to run them`.\n",
    "\n",
    "<br />\n",
    "\n",
    "The image below ilustrates how this technique works:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure style='text-align:center'>\n",
    "    <img style='border-radius:20px' src='./assets/2-collaborative-filtering.png' alt='Collaborative Filtering Diagram' />\n",
    "    <figcaption>Figure 1 - Collaborative Filtering Diagram. By <a href='https://www.analyticsvidhya.com/blog/2022/02/introduction-to-collaborative-filtering/'>Shivam Baldha - Introduction to Collaborative Filtering©</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "In this notebook, we are going further to User-Based technique and use K-Nearest Neighbors to find similar users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca383d-135b-425a-80f6-34d0d32c5e64",
   "metadata": {},
   "source": [
    "<h1 id='2-user-based-approach' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>🧑‍🤝‍🧑 | User-Based Approach</h1>\n",
    "\n",
    "In a few words, consider a person called user E, the `Collaborative Filtering User-Based Approach` works on finding similar users to user E and then recommendating to him similar items that the similar users liked.\n",
    "\n",
    "To do it, the Algorithm first calculates the similarity between the users using `Pearson Correlation, Cosine Similarity or other metric`, then predicts the rate user E would give to the animes that the most similar users have watched and recommends the most predicted, rated ones.\n",
    "\n",
    "For example, consider the following situation where we want to recommend movies to user E. The first step is to calculate the similarity of the others users to this one. The image below ilustrates the situation:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure style='text-align:center'>\n",
    "    <img style='border-radius:20px' src='./assets/6-collaborative-filtering-user-based.png' alt='Collaborative Filtering example using User-Based approach' />\n",
    "    <figcaption>Figure 2 - Board ilustrating the similarity of the users to user E. The indexes are the users, the columns are the movies and the users ratings to the movies and the last column is the similarity of the users to user E. The similarity has been calculated using Pearson Correlation. Besides, since user A and F have not rated movies that user E has been, their similarity is 0 (NaN). Since the similarity is being calculated to user E, user E has full similarity to itself; also, user D is totally different to user E due to the similarity be -1. By <a href='https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system'>Sibtesam Ahmed - Getting Started with a Movie Recommendation System©</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "After calculating the similarities, we have to predict the ratings that user E would give to the movies he hasn't rated and then, recommends to him the movies liked by the most similar users and that got the higher predicted ratings from user E. The following image pictures the results:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure style='text-align:center'>\n",
    "    <img style='border-radius:20px' src='./assets/7-collaborative-filtering-user-based.png' alt='Collaborative Filtering example results using User-Based approach' />\n",
    "    <figcaption>Figure 3 -Board ilustrating the results of the Collaborative Filtering. The predicted ratings of user E are marked with asterisks (*). The most similar users to user E are C and B. Probably, Avengers would be the recommended movie since its the movie that a similar user (B) has liked and got a high predicted rating to user E. By <a href='https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system'>Sibtesam Ahmed - Getting Started with a Movie Recommendation System©</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0805a-f6a8-4e8f-b5f6-bd40f7b137d1",
   "metadata": {},
   "source": [
    "<h1 id='3-k-nearest-neighbors' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>🧑‍🤝‍🧑 | K-Nearest Neighbors</h1>\n",
    "\n",
    "Instead of be using Pearson Correlation or Cosine Similarity to find similar users to a given one, we are going further and to apply `K-Nearest Neighbors` to do this task. This algorithm does one thing different than what has been done in the example from the previous section: instead of finding similar users, it considers that the users are grouped into clusters and its major goal is to find the most similar cluster to a given user.\n",
    "\n",
    "K-Nearest Neighbors works like this:\n",
    "\n",
    "> 1 - group the users into clusters (when the categories are known, we can stick into them. When the categories are unknown, we can use Unsupervisioned Machine Learning Algorithms, such as `K-Means Clustering`, to cluster the data);\n",
    "\n",
    "> 2 - for a given user, find the K nearest neighbors, being \"K\" the number of nearest neighbors to be considered;\n",
    "\n",
    "> 3 - when \"K\" is equals to 1, the given user is similar to the cluster of the unique nearest neighbor. When \"K\" is greater than 1, the given user is similar to the cluster of the most nearest neighbors belong. If there are a tie, we randomly choose one of the tied clusters to the given user be similar (picture that the user E has 5 nearest neighbors from cluster Red and 5 others from cluster Blue. Since both clusters has the same amount of users chosen as nearest neighbors to the given user, we randomly choose between Red and Blue to the very user be similar to).\n",
    "\n",
    "<br />\n",
    "\n",
    "The image below pictures an example of the clustering:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure style='text-align:center'>\n",
    "    <img style='border-radius:20px' src='./assets/8-k-nearest-neighbors.png' alt='Example of K-Nearest Neighbors Algorithm assigning a cluster to a given data point' />\n",
    "    <figcaption>Figure 4 - Example of K-Nearest Neighbors Algorithm assigning a cluster to a given data point. By <a href='https://www.youtube.com/watch?v=HVXime0nQeI'>StatQuest with Josh Starmer - StatQuest: K-nearest neighbors, Clearly Explained©</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "About the value of Nearest Neighbors (K) to be taken, we have to consider these information:\n",
    "\n",
    "> 1 - There is no phisical or biological way to determine the best value for \"K\", so you may have to try a few out values  before settling on one. Do this by pretending part of the training data is \"unknown\";\n",
    "\n",
    "> 2 - Low values for K, such as K=1 or K=2, can be noisy and subject to the effects of outliers;\n",
    "\n",
    "> 3 - Large values for K smooth over things, but you do not want to K be so large that a category with only a few samples in it will always be out voted by other categories.\n",
    "\n",
    "<br />\n",
    "\n",
    "For better explanations about how K-Nearest Neighbors and K-Means Clustering work, consider watching these two videos provided by [StatQuest with Josh Starmer](https://www.youtube.com/@statquest): [StatQuest: K-nearest neighbors, Clearly Explained](https://www.youtube.com/watch?v=HVXime0nQeI) and [StatQuest: K-means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148d054-4fcd-4780-a5c0-7000bf8611f0",
   "metadata": {},
   "source": [
    "<h1 id='4-k-nearest-neighbors-basic-vs-k-nearest-neighbors-with-means-vs-k-nearest-neighbors-with-z-score' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>🧑‍🤝‍🧑 | K-Nearest Neighbors Basic VS K-Nearest Neighbors With Means VS K-Nearest Neighbors With Z-Score</h1>\n",
    "\n",
    "The K-Nearest Neighbors (KNN) has some variations in its algorithms in other to get better results in specific situations, the main three ones are `Basic`, `With Means` and `With Z-Score` variations, let's see their description and settle which one to use in the project:\n",
    "\n",
    "> **K-Nearest Neighbors Basic** - `known as the vanilla version, this one is the first KNN Algorithm and has been explained in the previous section`;\n",
    "\n",
    "> **K-Nearest Neighbors With Means** - `this one is like the Basic version, with the addition of the mean ratings of each user in order to avoid outliers and give different weights to the users accordingly to their mean ratings`;\n",
    "\n",
    "> **K-Nearest Neighbors With Z-Score** - `this one is like the With Means version, with the addition of the Z-Score Normalization of each user mean rating`.\n",
    "\n",
    "Since assigning different weights to the users accordingly to their mean rating is a great idea to get more accurate results and since the ratings dataset is not normalized, we are going to stick to `K-Nearest Neighbors With Means` from now on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa895b6-1859-4920-9d04-cb641ab5d829",
   "metadata": {},
   "source": [
    "<h1 id='5-grid-search-cv-vs-randomized-search-cv' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>🧑‍🤝‍🧑 | Grid Search CV VS Randomized Search CV</h1>\n",
    "\n",
    "Okay, now that we already have chosen the model to find similar users, we have to find the best hyperparameters to it. Hyperparameters are those ones that the algorithm cannot learn by itself over the training step and it is a task for us to choose the best values in order to the model provide the best results.\n",
    "\n",
    "For K-Nearest Neighbors, the hyperparameters we are going to consider are the `Similarity Metric, User-Based Approach, Minimum Number of Similar Items and Minimum and Maximum Number of Nearest Neighbors`.\n",
    "\n",
    "Fortunately, there are some techniques we can use instead of trying a bunch of random values, such as `Grid Search CV` and `Randomized Search CV`, being:\n",
    "\n",
    "> **Grid Search CV** - `tests the permutation of all hyperparameters and returns the values that made the model got the best results. This approach is recommended for small datasets`;\n",
    "\n",
    "> **Randomized Search CV** - `tests some random permutations of the hyperparameters and returns the values that made the model got the best results. This approach is recommended for large datasets`.\n",
    "\n",
    "For a better understanding, consider we have two hyperparameters $A=[1, 2, 3]$ and $B=[4, 5, 6]$. In Grid Search CV, all permutations are tested, that is, all A-B pairs into $pairs = [(1, 4), (1, 5), (1, 6), (2, 4), (2, 5), (2, 6), (3, 4), (3, 5), (3,6)]$ are tested; whereas in Randomized Search CV, only a few randomly chosen A-B pairs are tested.\n",
    "\n",
    "Since our ratings dataset is kind of large - over than 23 million observations!! -, we are going to stick to `Randomized Search CV`.\n",
    "\n",
    "Besides, notice that the Recommendation Problem has been turned into an Optimization Problem, where we should find out the Unsupervisioned Machine Learning Algorithm and the Hyperparameters Values that fit the problem best. Now, let's go to the code!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728b40f-30c2-41c4-b7b8-d277dd5cc02a",
   "metadata": {},
   "source": [
    "<h1 id='6-hands-on' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>🧑‍🤝‍🧑 | Hands-on</h1>\n",
    "\n",
    "```\n",
    "- Settings\n",
    "- Reading Datasets\n",
    "- Dropping Variables\n",
    "- Getting Random Observations for Hyperparameter Tuning\n",
    "- Finding Best Hyperparameters for the Model\n",
    "- Splitting Dataset into Training and Validation\n",
    "- Training the Model\n",
    "- Recommendations\n",
    "```\n",
    "\n",
    "> **OBS.:** since Surprise Package only works with ratings from 0 to 5 and the animes dataset works with ratings from 0 to 10, we have to divide the animes dataset ratings by 2 in order to the Surprise Predictions be accordingly to the range from 0 to 5. After that, we have to multiply the Surprise Predictions by 2 in order to the predictions fit the animes dataset ratings when returning the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e26f0-154e-4d3c-9b82-713e98bd5f24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**- Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b128c5a6-5cae-4c27-9161-2df224901510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# ---- Imports ----\n",
    "# -----------------\n",
    "from collections import defaultdict  # pip install collections\n",
    "import inflect                       # pip install inflect\n",
    "import numpy as np                   # pip install numpy\n",
    "import pandas as pd                  # pip install pandas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ---- Surprise Imports ----\n",
    "# --------------------------\n",
    "#\n",
    "# pip install scikit-surprise\n",
    "#\n",
    "# If you get any problems installing the package, follow the steps in this Stack Overflow link:\n",
    "# - https://stackoverflow.com/questions/44951456/pip-error-microsoft-visual-c-14-0-is-required\n",
    "#\n",
    "from surprise import accuracy\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.dataset import Dataset\n",
    "from surprise.model_selection.validation import cross_validate\n",
    "from surprise.model_selection import RandomizedSearchCV\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.reader import Reader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# ---- Constants ----\n",
    "# -------------------\n",
    "DATASETS_PATH = ('./datasets')\n",
    "INFLECT_ENGINE = (inflect.engine())\n",
    "SEED = (20240106)\n",
    "SEED2 = (20240107)\n",
    "SURPRISE_READER = (Reader())\n",
    "\n",
    "HYPERPARAMETER_TUNING_SAMPLE_SIZE = 15_000 # my laptop crashes with more than 15,000 observations. I need more RAM :( (currently I have 12GB)\n",
    "TRAINING_DF_SIZE = (0.70)\n",
    "VALIDATION_DF_SIZE = (0.30)\n",
    "\n",
    "SIMILARITY_OPTIONS = {\n",
    "    'name': ['cosine', 'pearson', 'pearson_baseline', 'msd'] # similarity metrics: the best one will be used for recommendations;\n",
    "    , 'user_based': [True] # True: User-Based Approach; False: Item-Based Approach;\n",
    "    , 'min_support': [3, 4, 5] # User-Based: minimum number of similar items to be considered; Item-Based: minimum number of similar users to be considered.\n",
    "}\n",
    "KNN_PARAMS = {\n",
    "    'k': [30, 40, 50]              # list of maximum nearest neighbors to be considered: the best one will be used for recommendations;\n",
    "    , 'min_k': [20, 25, 30]        # minimum number of nearest neighbors to be considered;\n",
    "    , 'sim_options': SIMILARITY_OPTIONS # similarity parameters;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------\n",
    "# ---- Settings ----\n",
    "# ------------------\n",
    "np.random.seed(SEED)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# ---- Functions ----\n",
    "# -------------------\n",
    "def find_best_hyperparameters_values(model, search_cv, parameters, dataset):\n",
    "    \"\"\"\n",
    "    \\ Descrition:\n",
    "        - creates a parameter Search CV to find the best score, hyparameters values and estimators for a given\n",
    "    K-Nearest Neighbors model;\n",
    "        - returns the best model and its score, hyperparameters and estimators.\n",
    "    \n",
    "    \\ Parameters:\n",
    "        - model: Chosen Classification model. For this notebook, K-Nearest Neighbors With Means is the chosen one;\n",
    "        - search-cv: Surprise Parameter Search. For this notebook, Randomized Search CV is the chosen one;\n",
    "        - parameters: dictionary of KNN Parameters;\n",
    "        - dataset: Surprise DataFrame.\n",
    "    \"\"\"\n",
    "    classification_model = search_cv(\n",
    "        model\n",
    "        , parameters\n",
    "        , n_jobs=-1                 # number of CPU cores used on models' training and validation (-1 all cores are used)\n",
    "        , measures=['rmse']         # evaluation metrics\n",
    "        , cv=3                      # number of folds for cross-validation\n",
    "        , pre_dispatch='1*n_jobs'   # number of dispatches for each job to process simultaneously\n",
    "        , random_state=SEED\n",
    "    )\n",
    "    classification_model.fit(dataset)\n",
    "    \n",
    "    print(f'- Best Score: {classification_model.best_score}')\n",
    "    print(f'- Best Parameters: {classification_model.best_params}')\n",
    "    print(f'- Best Estimator: {classification_model.best_estimator}')\n",
    "    \n",
    "    return classification_model\n",
    "\n",
    "\n",
    "def get_recommendations(predictions_df, animes_df, maximum_number_of_recommendations=10):\n",
    "    \"\"\"\n",
    "    \\ Description:\n",
    "        - Maps the Predictions for Each User;\n",
    "        - Sorts the Predictios for Each User;\n",
    "        - Creates a List Containing the K-Nearest Neighbors and Highest Predicted Rating Ones;\n",
    "        - Returns the List.\n",
    "    \n",
    "    \\ Parameters:\n",
    "        - Predictions DF: Surprise DataFrame;\n",
    "        - Animes DF: Pandas DataFrame;\n",
    "        - Maximum Number of Recommendations: Integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mapping the Predictions for Each User\n",
    "    #\n",
    "    # - since Surprise Package only works with ratings from 0 to 5 and\n",
    "    # the animes dataset works with ratings from 0 to 10, we have to\n",
    "    # multiply the 'estimated (est)' value by 2, in order to the Surprise\n",
    "    # predictions fit into the animes dataset ratings.\n",
    "    #\n",
    "    top_n_recommendations = defaultdict(list)\n",
    "    for uuid, id, true_r, est, _ in predictions_df: top_n_recommendations[uuid].append((id, est * 2))\n",
    "    \n",
    "    # Sorting Predictions for Each User and Retrieving the K-Nearest Neighbors and Highest Ones\n",
    "    for uuid, user_ratings in top_n_recommendations.items():\n",
    "        user_ratings.sort(key=lambda rating: rating[1], reverse=True)\n",
    "        top_n_recommendations[uuid] = user_ratings[:maximum_number_of_recommendations]\n",
    "    \n",
    "    return top_n_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee848e7-bef0-4557-859f-57174f4e9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# ---- Classes ----\n",
    "# -----------------\n",
    "class collaborative_filtering_user_based_approach():\n",
    "    \"\"\"\n",
    "    This class apply Collaborative Filtering with User-Based Approach to recommend 'n' animes to a given user.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, training_df, validation_df, full_dataset, animes_df):\n",
    "        \"\"\"\n",
    "        \\ Description:\n",
    "            - Class Constructor.\n",
    "        \n",
    "        \\ Parameters:\n",
    "            - Model: Surprise Prediction Model. K-Nearest Neighbors With Means is the chosen one for this notebook;\n",
    "            - Training DF: Surprise DataFrame;\n",
    "            - Validation DF: Surprise DataFrame;\n",
    "            - Full Dataset: Surprise DataFrame (Merge of: Training DF and Validation DF);\n",
    "            - Animes DF:  Pandas DataFrame.\n",
    "            \n",
    "        \\ Other Attributes:\n",
    "            - Predictions Validations: Surprise DataFrame;\n",
    "            - Top Recommendations: Surprise DataFrame;\n",
    "            - Recommendations DF: Pandas DataFrame.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.training_df = training_df\n",
    "        self.validation_df = validation_df\n",
    "        self.full_dataset = full_dataset\n",
    "        self.animes_df = animes_df\n",
    "        \n",
    "        self.predictions_validations = None\n",
    "        self.top_recommendations = None\n",
    "        self.recommendations_df = None\n",
    "        \n",
    "    def fit_and_predict(self, maximum_number_of_recommendations, verbose=True):\n",
    "        \"\"\"\n",
    "        \\ Description:\n",
    "            - Applies training, validation and evaluation steps;\n",
    "            - Gets the top 'n' recommendations;\n",
    "            - Returns the top 'n' recommendations as a Pandas DataFrame.\n",
    "        \n",
    "        \\ Parameters:\n",
    "            - Maximum Number of Recommendations: Integer;\n",
    "            - Verbose: Boolean.\n",
    "        \"\"\"\n",
    "        # ---- Fitting Step ----\n",
    "        if verbose:\n",
    "            fitting_step_title = '** Fitting Step **'\n",
    "            print('*' * len(fitting_step_title))\n",
    "            print(fitting_step_title)\n",
    "            print('*' * len(fitting_step_title))\n",
    "            print('\\n')\n",
    "            \n",
    "        self.model.fit(self.training_df)\n",
    "        \n",
    "        # ---- Validation Step -----\n",
    "        if verbose:\n",
    "            prediction_step_title = '** Prediction Step **'\n",
    "            print('\\n\\n')\n",
    "            print('*' * len(prediction_step_title))\n",
    "            print(prediction_step_title)\n",
    "            print('*' * len(prediction_step_title))\n",
    "            print('\\n')\n",
    "            \n",
    "        self.predictions_validations = self.model.test(self.validation_df)\n",
    "\n",
    "        # ---- Evaluation Step ----\n",
    "        if verbose:\n",
    "            evaluation_step_title = '** Evaluation Step **'\n",
    "            print('\\n\\n')\n",
    "            print('*' * len(evaluation_step_title))\n",
    "            print(evaluation_step_title)\n",
    "            print('*' * len(evaluation_step_title))\n",
    "            print('\\n')\n",
    "        \n",
    "        rmse_evaluation = round(accuracy.rmse(self.predictions_validations), 4)\n",
    "        print(f'- Root Mean Squared Error (RMSE) for the predictions: {rmse_evaluation}')\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        # ---- Getting Recommendations ----\n",
    "        if verbose:\n",
    "            predicting_recommendations_step_title = '** Predicting Recommendations Step **'\n",
    "            print('\\n\\n')\n",
    "            print('*' * len(predicting_recommendations_step_title))\n",
    "            print(predicting_recommendations_step_title)\n",
    "            print('*' * len(predicting_recommendations_step_title))\n",
    "            print('\\n')\n",
    "            \n",
    "        self.top_recommendations = get_recommendations(self.predictions_validations, self.animes_df)\n",
    "        self.recommendations_df = pd.DataFrame(columns=['user_id', 'anime_id', 'predicted_rating'])\n",
    "        \n",
    "        for item in self.top_recommendations:\n",
    "            current_recommendation_df = pd.DataFrame(self.top_recommendations[item], columns=['anime_id', 'predicted_rating'])\n",
    "            current_recommendation_df['user_id'] = item\n",
    "            recommendation_variables = current_recommendation_df.columns.tolist()\n",
    "            recommendation_variables = recommendation_variables[-1:] + recommendation_variables[:-1]\n",
    "            current_recommendation_df = current_recommendation_df[recommendation_variables]\n",
    "            self.recommendations_df = pd.concat([self.recommendations_df, current_recommendation_df], axis=0)\n",
    "        \n",
    "        # ---- Return ----\n",
    "        print('Fitting and Prediction Done 😉👍')\n",
    "        print('\\n')\n",
    "        return rmse_evaluation\n",
    "    \n",
    "    def cross_validation(self, verbose=True):\n",
    "        \"\"\"\n",
    "        \\ Description:\n",
    "            - Applies Cross-Validation and its results evaluation using RMSE;\n",
    "            - Returns the evaluation as a Pandas DataFrame.\n",
    "        \n",
    "        \\ Parameters:\n",
    "            - Verbose: Boolean.\n",
    "        \"\"\"\n",
    "        # ---- Cross-Validation Step ----\n",
    "        if verbose:\n",
    "            cross_validation_step_title = '** Cross-Validation Step **'\n",
    "            print('*' * len(cross_validation_step_title))\n",
    "            print(cross_validation_step_title)\n",
    "            print('*' * len(cross_validation_step_title))\n",
    "            print('\\n')\n",
    "            \n",
    "        cross_validation_results = cross_validate(self.model, self.full_dataset, n_jobs=-1)\n",
    "        cross_validation_results = round(cross_validation_results['test_rmse'].mean(), 4)\n",
    "            \n",
    "        # ---- Evaluation Step ----\n",
    "        if verbose:\n",
    "            validation_step_title = '** Validation Step **'\n",
    "            print('\\n\\n')\n",
    "            print('*' * len(validation_step_title))\n",
    "            print(validation_step_title)\n",
    "            print('*' * len(validation_step_title))\n",
    "            print('\\n')\n",
    "        \n",
    "        print(f'- Mean Cross-Validation Root Mean Squared Error (RMSE): {cross_validation_results}')\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        # ---- Return ----\n",
    "        print('Cross-Validation Done 😉👍')\n",
    "        print('\\n')\n",
    "        return cross_validation_results\n",
    "    \n",
    "    def recommend(self, user_id, maximum_number_of_recommendations=10, verbose=True):\n",
    "        \"\"\"\n",
    "        \\ Description:\n",
    "            - Gets the 'n' recommended items;\n",
    "            - Gets the recommended animes info;\n",
    "            - Returns the recommended animes info and the predicted rating as a Pandas DataFrame.\n",
    "        \n",
    "        \\ Parameters:\n",
    "            - User ID: Integer;\n",
    "            - Maximum Number of Recommendations: Integer;\n",
    "            - Verbose: Boolean.\n",
    "        \"\"\"\n",
    "        # ---- Recommendation Step ----\n",
    "        if verbose:\n",
    "            recommendation_step_title = '** Recommendations Step **'\n",
    "            print('*' * len(recommendation_step_title))\n",
    "            print(recommendation_step_title)\n",
    "            print('*' * len(recommendation_step_title))\n",
    "            print('\\n')\n",
    "        \n",
    "        recommendations_df = self.recommendations_df.loc[self.recommendations_df['user_id'] == user_id] \\\n",
    "            .head(maximum_number_of_recommendations)\n",
    "        \n",
    "        # ---- Formatting Animes Recommendation DataFrame ----\n",
    "        recommended_anime_ids = recommendations_df.anime_id.to_list()\n",
    "        recommended_animes_df = self.animes_df.loc[recommended_anime_ids]\n",
    "        recommended_animes_df['predicted_rating'] = recommendations_df['predicted_rating'].to_list()\n",
    "        \n",
    "        # ---- Return ----\n",
    "        if verbose: display(recommended_animes_df)\n",
    "        print('Recommendations Done, Enjoy 😉👍')\n",
    "        print('\\n')\n",
    "        \n",
    "        return recommended_animes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a3816-7ca5-4b4b-994b-e5afef38d3d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**- Reading Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b60c7dd-bd53-4320-9b0b-b95bf11fd08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of Observations: 23748 (twenty-three thousand, seven hundred and forty-eight)\n",
      "- Number of Variables: 5 (five)\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>genres</th>\n",
       "      <th>is_hentai</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cowboy bebop</td>\n",
       "      <td>8.75</td>\n",
       "      <td>award winning, action, sci-fi</td>\n",
       "      <td>0</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/4/19644.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cowboy bebop tengoku no tobira</td>\n",
       "      <td>8.38</td>\n",
       "      <td>action, sci-fi</td>\n",
       "      <td>0</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/1439/93480.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trigun</td>\n",
       "      <td>8.22</td>\n",
       "      <td>adventure, action, sci-fi</td>\n",
       "      <td>0</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/7/20310.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>witch hunter robin</td>\n",
       "      <td>7.25</td>\n",
       "      <td>mystery, supernatural, action, drama</td>\n",
       "      <td>0</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/10/19969.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bouken ou beet</td>\n",
       "      <td>6.94</td>\n",
       "      <td>adventure, supernatural, fantasy</td>\n",
       "      <td>0</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/7/21569.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  score  \\\n",
       "id                                          \n",
       "1                     cowboy bebop   8.75   \n",
       "5   cowboy bebop tengoku no tobira   8.38   \n",
       "6                           trigun   8.22   \n",
       "7               witch hunter robin   7.25   \n",
       "8                   bouken ou beet   6.94   \n",
       "\n",
       "                                  genres  is_hentai  \\\n",
       "id                                                    \n",
       "1          award winning, action, sci-fi          0   \n",
       "5                         action, sci-fi          0   \n",
       "6              adventure, action, sci-fi          0   \n",
       "7   mystery, supernatural, action, drama          0   \n",
       "8       adventure, supernatural, fantasy          0   \n",
       "\n",
       "                                                  image_url  \n",
       "id                                                           \n",
       "1      https://cdn.myanimelist.net/images/anime/4/19644.jpg  \n",
       "5   https://cdn.myanimelist.net/images/anime/1439/93480.jpg  \n",
       "6      https://cdn.myanimelist.net/images/anime/7/20310.jpg  \n",
       "7     https://cdn.myanimelist.net/images/anime/10/19969.jpg  \n",
       "8      https://cdn.myanimelist.net/images/anime/7/21569.jpg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Reading Animes Dataset ----\n",
    "animes_df = pd.read_csv(f'{DATASETS_PATH}/anime-transformed-dataset-2023.csv', index_col='id')[\n",
    "    ['title', 'score', 'genres', 'is_hentai', 'image_url']\n",
    "]\n",
    "\n",
    "print(f'- Number of Observations: {animes_df.shape[0]} ({INFLECT_ENGINE.number_to_words(animes_df.shape[0])})')\n",
    "print(f'- Number of Variables: {animes_df.shape[1]} ({INFLECT_ENGINE.number_to_words(animes_df.shape[1])})')\n",
    "print('---')\n",
    "\n",
    "animes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eea99b2-6e52-4f53-9dc7-34e2b2da11f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of Observations: 23796586 (twenty-three million, seven hundred and ninety-six thousand, five hundred and eighty-six)\n",
      "- Number of Variables: 5 (five)\n",
      "---\n",
      "- Number of Unique Users: 264067 (two hundred and sixty-four thousand and sixty-seven)\n",
      "- Number of Unique Animes: 16380 (sixteen thousand, three hundred and eighty)\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>anime_title</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>xinil</td>\n",
       "      <td>21</td>\n",
       "      <td>one piece</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>xinil</td>\n",
       "      <td>48</td>\n",
       "      <td>hack sign</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>xinil</td>\n",
       "      <td>320</td>\n",
       "      <td>a kite</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>xinil</td>\n",
       "      <td>49</td>\n",
       "      <td>aa megami-sama</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>xinil</td>\n",
       "      <td>304</td>\n",
       "      <td>aa megami-sama movie</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id username  anime_id           anime_title  rating\n",
       "0        1    xinil        21             one piece       9\n",
       "1        1    xinil        48             hack sign       7\n",
       "2        1    xinil       320                a kite       5\n",
       "3        1    xinil        49        aa megami-sama       8\n",
       "4        1    xinil       304  aa megami-sama movie       8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Reading Ratings Dataset ----\n",
    "ratings_df = pd.read_csv(f'{DATASETS_PATH}/users-scores-transformed-2023.csv')\n",
    "\n",
    "print(f'- Number of Observations: {ratings_df.shape[0]} ({INFLECT_ENGINE.number_to_words(ratings_df.shape[0])})')\n",
    "print(f'- Number of Variables: {ratings_df.shape[1]} ({INFLECT_ENGINE.number_to_words(ratings_df.shape[1])})')\n",
    "print('---')\n",
    "\n",
    "print(f'- Number of Unique Users: {ratings_df.user_id.nunique()} ({INFLECT_ENGINE.number_to_words(ratings_df.user_id.nunique())})')\n",
    "print(f'- Number of Unique Animes: {ratings_df.anime_id.nunique()} ({INFLECT_ENGINE.number_to_words(ratings_df.anime_id.nunique())})')\n",
    "print('---')\n",
    "\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60f928-d81e-4fc8-b20b-d0ad2f55c146",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**- Dropping Variables**\n",
    "\n",
    "For Surprise package, only three variables are needed: the user id, the anime id and the rating the user gave to the anime. Thus, we have to drop the user name and anime title variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090c28b3-991b-4cd2-86ec-be832877940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Dropping Variables ----\n",
    "variables_to_keep = ['user_id', 'anime_id', 'rating']\n",
    "ratings_df = ratings_df[variables_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d4dc4-9c9d-4c0f-a954-0b4b5c15e6af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**- Getting Random Observations for Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248125b5-035a-4edf-ab01-d938def9491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of Observations for Hyperparameter Tuning: 15000 (fifteen thousand)\n"
     ]
    }
   ],
   "source": [
    "# ---- Getting Random Observations for Hyperparameter Tuning ----\n",
    "#\n",
    "# - since Surprise Package only works with ratings from 0 to 5 and\n",
    "# the animes dataset works with ratings from 0 to 10, we have to\n",
    "# divide the 'rating' variable by 2, in order to the anime dataset\n",
    "# ratings fit into Surprise Predictions.\n",
    "#\n",
    "temp_ratings_df = ratings_df.copy()\n",
    "temp_ratings_df.rating = temp_ratings_df.rating.apply(lambda rating: rating / 2)\n",
    "\n",
    "hyperparameter_tuning_df = temp_ratings_df.sample(HYPERPARAMETER_TUNING_SAMPLE_SIZE, random_state=SEED)\n",
    "hyperparameter_tuning_df = Dataset.load_from_df(hyperparameter_tuning_df, SURPRISE_READER)\n",
    "\n",
    "print(\n",
    "    f'- Number of Observations for Hyperparameter Tuning: {HYPERPARAMETER_TUNING_SAMPLE_SIZE}'\n",
    "    f' ({INFLECT_ENGINE.number_to_words(HYPERPARAMETER_TUNING_SAMPLE_SIZE)})'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b62e56-2828-4914-9d83-b4e8fb1a1501",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**- Finding Best Hyperparameters for the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd45fd2d-b039-40c6-b142-e412676ffd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Best Score: {'rmse': 0.8510943485466242}\n",
      "- Best Parameters: {'rmse': {'k': 30, 'min_k': 25, 'sim_options': {'name': 'pearson_baseline', 'user_based': True, 'min_support': 5}}}\n",
      "- Best Estimator: {'rmse': <surprise.prediction_algorithms.knns.KNNWithMeans object at 0x000002082B76BD60>}\n"
     ]
    }
   ],
   "source": [
    "# ---- Finding the Best Hyperparameters for the Model ----\n",
    "#\n",
    "# - using Randomized Search CV in order to find the best Hyperparameters Values.\n",
    "#\n",
    "hyperparameters_tuning_values = find_best_hyperparameters_values(\n",
    "    model=KNNWithMeans\n",
    "    , search_cv=RandomizedSearchCV\n",
    "    , parameters=KNN_PARAMS\n",
    "    , dataset=hyperparameter_tuning_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdbaa73-73e5-445f-9224-62b9058ac953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Finding Best Hyperparameters for the Model ----\n",
    "#\n",
    "# - getting the model with the best parameters.\n",
    "#\n",
    "chosen_knn_with_means_model = hyperparameters_tuning_values.best_estimator['rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b210a-d9f8-4bf0-9b0f-a6fb48492a1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**- Splitting Dataset into Training and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd48dadb-715d-49e9-993d-ba38c80c8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Splitting Dataset into Training and Validation ----\n",
    "#\n",
    "# - converting Pandas DataFrame into Surprise DataFrame\n",
    "#\n",
    "ratings_surprise_df = Dataset.load_from_df(\n",
    "    temp_ratings_df.sample(HYPERPARAMETER_TUNING_SAMPLE_SIZE, random_state=SEED2)\n",
    "    , SURPRISE_READER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbba1bb0-0d25-4e75-aa6a-537f17f966a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Splitting Dataset into Training and Validation ----\n",
    "#\n",
    "# - deleting some dataframes from the memory since we are going to use 'ratings_surprise_df' from now on;\n",
    "# - datasets to delete:\n",
    "#    \\ ratings_df;\n",
    "#    \\ temp_ratings_df;\n",
    "#    \\ hyperparameter_tuning_df.\n",
    "#\n",
    "ratings_df = None\n",
    "temp_ratings_df = None\n",
    "hyperparameter_tuning_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "148d6210-b5a3-4b12-9bb4-79d3eb61b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Splitting Dataset into Trainig and Validation ----\n",
    "#\n",
    "# - training: 70%\n",
    "# - validation: 30%\n",
    "#\n",
    "training_surprise_df, validation_surprise_df = train_test_split(\n",
    "    data=ratings_surprise_df\n",
    "    , train_size=TRAINING_DF_SIZE\n",
    "    , test_size=VALIDATION_DF_SIZE\n",
    "    , random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a8cc5d-4ec6-45b0-81a8-d10dd739ce30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**- Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b59edaa-3d76-4f26-a961-2ed47e16a77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************\n",
      "** Fitting Step **\n",
      "******************\n",
      "\n",
      "\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "\n",
      "\n",
      "*********************\n",
      "** Prediction Step **\n",
      "*********************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*********************\n",
      "** Evaluation Step **\n",
      "*********************\n",
      "\n",
      "\n",
      "RMSE: 0.8622\n",
      "- Root Mean Squared Error (RMSE) for the predictions: 0.8622\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*************************************\n",
      "** Predicting Recommendations Step **\n",
      "*************************************\n",
      "\n",
      "\n",
      "Fitting and Prediction Done 😉👍\n",
      "\n",
      "\n",
      "***************************\n",
      "** Cross-Validation Step **\n",
      "***************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*********************\n",
      "** Validation Step **\n",
      "*********************\n",
      "\n",
      "\n",
      "- Mean Cross-Validation Root Mean Squared Error (RMSE): 0.8481\n",
      "\n",
      "\n",
      "\n",
      "Cross-Validation Done 😉👍\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Training the Model ----\n",
    "#\n",
    "# - creating the model;\n",
    "# - fitting and predicting;\n",
    "# - cross-validating datas.\n",
    "#\n",
    "user_based_recommender = collaborative_filtering_user_based_approach(\n",
    "    chosen_knn_with_means_model\n",
    "    , training_surprise_df\n",
    "    , validation_surprise_df\n",
    "    , ratings_surprise_df\n",
    "    , animes_df\n",
    ")\n",
    "\n",
    "model_rmse = user_based_recommender.fit_and_predict(maximum_number_of_recommendations=10, verbose=True)\n",
    "model_cross_validation_rmse = user_based_recommender.cross_validation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30aa432-4091-43af-ac31-c1f5fb77f40a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**- Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44211a7-216f-4346-a334-a91ab23029ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************\n",
      "** Recommendations Step **\n",
      "**************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>genres</th>\n",
       "      <th>is_hentai</th>\n",
       "      <th>image_url</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18465</th>\n",
       "      <td>genshiken nidaime</td>\n",
       "      <td>7.43</td>\n",
       "      <td>comedy</td>\n",
       "      <td>0</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/11/52935.jpg</td>\n",
       "      <td>7.618476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  score  genres  is_hentai  \\\n",
       "id                                                   \n",
       "18465  genshiken nidaime   7.43  comedy          0   \n",
       "\n",
       "                                                   image_url  predicted_rating  \n",
       "id                                                                              \n",
       "18465  https://cdn.myanimelist.net/images/anime/11/52935.jpg          7.618476  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations Done, Enjoy 😉👍\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Recommendations ----\n",
    "user_based_top_10_animes = user_based_recommender.recommend(\n",
    "    user_id=388458\n",
    "    , maximum_number_of_recommendations=10\n",
    "    , verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d403658-2c19-4e8e-8c48-4efe785daaa7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>📫 | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
